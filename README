
Tarea 3 SD: Cassandra

Integrantes: Abel Sierra

Dado que el proyecto esta en docker, lo unico que se debe hacer una vez clonado el repositorio es lo siguietnete:

docker-compose build
docker-compose up

El compose esta basado en los entregados en los siguientes repositorios:

# https://digitalis.io/blog/containerized-cassandra-cluster-for-local-testing
# https://github.com/bitnami/bitnami-docker-cassandra/blob/master/docker-compose-cluster.yml


Una vez hecho lo anterior, se crearan los 3 contenedores de Cassandra solicitados junto a un container adicional para montar la API.

La API se encuentra montada en Python debido a las dificultades de conexion encontradas al usar JS. Tristemente, las mismas dificultades de conexion fueron encontradas al trabajar con Python...

Las imagenes de Cassandra utilizadas para el proyecto son las de Bitnami puesto que las otras imagenes agotaban la memoria disponible en el equipo al montar 3 instancias distintas.

Las diversas rutas de la api son las siguientes:


La ruta http://localhost:5000/delete recibe una id de receta ingresada en una de las tablas y la borra.


La ruta http://localhost:5000/edit recibe una id de receta a modificiar, ademas de los parametros a modificar y los nuevos valores respectivos


Las rutas http://localhost:5000/res y http://localhost:5000/pas devuelven los contenidos de las tablas recetas y pacientes respectivamente. Esto con el fin de comprobar
la correcta creacion de cada uno de los inputs en dichas tablas (para modificar y eliminar recetas ademas de conocer los pacientes previamente existentes)


Finalmente, la ruta http://localhost:5000/create ingresa recetas en la base de datos. En caso de no existir el usuario asociado a dicha receta, tambien lo crea.


Los datos requeridos para utilizar las funciones asociadas a cada una de estas direccions son envidadas como formulario y en forma de POST.


Tristemente, todo el funcionamiento es teorico ya que no se pudo realizar la conexion al Cluster de Cassandra.


-----------------------
-----------------------


Preguntas Asociadas a la Actividad:

1)

La topologia de casandra es una de anillo, donde ninguno de los miembros es mas importante que los demas.
Los miembros del anillo se comunican mediante un protocolo de Gossip. Este protocolo es un intercambio de mensajes de estado entre un nodo y a lo mas 3 nodos de la estructura.
Estos mensajes ocurren cada segundo e incluyen informacion del nodo y de los nodos con los que este se ha comunicado.

Al recebir una consulta, el nodo receptor encamina la consulta a los nodos responsables de la informacion consultada. Esto es necesario puesto que no toda la informacion esta en todos los nodos.

Cassandra es tolerante a fallos debido a la duplicacion de los datos con los que cuenta. Obviamente siempre dependera del grado de replicacion de la informacion,
pero se esperaria que cada dato este en una cantidad minima de tablas en un sistema funcional

En Clusters pequeños resulta util, sin embargo, debido a la gran cantidad de mensajes (Gossip) y a la limitacion de nodos a los que se les puede realizar dicho proceso,
existe un numero maximo de Clusters en los cuales no existira problemas, posteriormente, el proceso se vuelve ineficiente

Al realizar una consulta, el nodo consultado (cordinador) se vuelve una especie de balanceador de carga puesto que este sera el encargado de enviar la consulta al nodos correspondiente


2)

Los metodos son:

SimpleStrategy: Replica los datos cuando el Data Center es "local" (Es decir, los nodos del Cluster estan en un unico Data Center)
NetworkTopologyStrategy: Utilizado al usar multiples Data Center (Los Clusters se encuentran distribuidos). Tambien existe Old Network Topology Strategy, una version deprecada de este metodo

Dado que se trabaja con un Cluster local, basta con utilizar SimpleStrategy para solucionar el problema actual

3)

En un nivel pequeño Cassandra deberia ser capaz de suplir las necesidades del problema, sin embargo, como se dijo antes, Cassandra puede tener problemas en Clusters de gran volumen

Si bien Cassandra permite escalar la memoria de los nodos, dado que buscamos distribuir al escalar, se necesitarian multiples Data Center dado que no tiene mucho sentido tener un Cluster de forma local si se busca tolerancia a fallos

Obviamente se deben solucionar los problemas de conexion entre la API y el Cluster antes de poner esta solucion en marcha.
Ademas, a nivel personal buscaria replicar los nodos en al menos otro Data Center puesto que en mi sector la luz se va de forma periodica (?)

